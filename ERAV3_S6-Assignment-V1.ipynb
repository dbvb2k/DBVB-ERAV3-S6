{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2ecdaae1-bd11-4a74-9572-e655a778a3e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import random_split, Subset\n",
    "from torchsummary import summary\n",
    "# from S6_model import get_model, save_model\n",
    "# from S6_Newmodel import get_model, save_model\n",
    "from Newmodel import get_model, save_model\n",
    "# from ChkModel import get_model, save_model\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import argparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8a2be9f1-d52e-4d58-abb0-03f2e85fb622",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_transform(apply_augmentation=False):\n",
    "    \"\"\"Get data transformation pipeline with enhanced augmentation\"\"\"\n",
    "    transforms_list = [\n",
    "        transforms.ToTensor(),  # Convert to tensor first\n",
    "        transforms.Normalize((0.1307,), (0.3081,))\n",
    "    ]\n",
    "    \n",
    "    if apply_augmentation:\n",
    "        transforms_list = [\n",
    "            transforms.RandomAffine(\n",
    "                degrees=10,\n",
    "                translate=(0.1, 0.1),\n",
    "                scale=(0.9, 1.1),\n",
    "                fill=0,\n",
    "            ),\n",
    "            transforms.RandomPerspective(distortion_scale=0.2, p=0.5),\n",
    "        ] + transforms_list  # Add base transforms after augmentation\n",
    "        \n",
    "        # Add RandomErasing after converting to tensor\n",
    "        transforms_list.append(transforms.RandomErasing(p=0.1))\n",
    "    \n",
    "    return transforms.Compose(transforms_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1df6bb2f-de76-47a7-90ba-15abc0b67fc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_directories():\n",
    "    \"\"\"Create necessary directories\"\"\"\n",
    "    if not os.path.exists('models'):\n",
    "        os.makedirs('models')\n",
    "        print(\"Created 'models' directory for saving checkpoints\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0df7161a-6a1c-4356-beb6-6cd392369ea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_device():\n",
    "    \"\"\"Set up and return the device to use\"\"\"\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "    return device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "21591f1d-01da-40df-a800-41ef8943370b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add this at the start of your notebook or training script\n",
    "def set_seed(seed=42):\n",
    "    \"\"\"Set seeds for reproducibility\"\"\"\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "276f917b-12e0-4225-8692-e5415f7cd48d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use it before loading data\n",
    "set_seed(42)\n",
    "\n",
    "def load_data(use_augmentation, batch_size=128):\n",
    "    \"\"\"Load and prepare data loaders with shuffled split\"\"\"\n",
    "    train_transform = get_transform(apply_augmentation=use_augmentation)\n",
    "    test_transform = get_transform(apply_augmentation=False)\n",
    "    \n",
    "    # Load full training dataset\n",
    "    full_train_dataset = datasets.MNIST('./data', train=True, download=True, transform=train_transform)\n",
    "    test_dataset = datasets.MNIST('./data', train=False, transform=test_transform)\n",
    "    \n",
    "    # Generate shuffled indices\n",
    "    num_train = len(full_train_dataset)\n",
    "    indices = torch.randperm(num_train)  # Creates a random permutation of indices\n",
    "    \n",
    "    # Split indices\n",
    "    train_size = 50000\n",
    "    train_indices = indices[:train_size]\n",
    "    val_indices = indices[train_size:]\n",
    "    \n",
    "    print(f\"Split details:\")\n",
    "    print(f\"- Total dataset size: {num_train}\")\n",
    "    print(f\"- Training set: {len(train_indices)} samples (randomly selected)\")\n",
    "    print(f\"- Validation set: {len(val_indices)} samples (randomly selected)\")\n",
    "    \n",
    "    # Create subsets using shuffled indices\n",
    "    train_dataset = Subset(full_train_dataset, train_indices)\n",
    "    val_dataset = Subset(\n",
    "        datasets.MNIST('./data', train=True, download=False, transform=test_transform),\n",
    "        val_indices\n",
    "    )\n",
    "    \n",
    "    # Create data loaders\n",
    "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)  # Additional shuffling during training\n",
    "    val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, shuffle=False)     # No need to shuffle validation set\n",
    "    test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    return train_loader, val_loader, test_loader, len(train_dataset), len(val_dataset), len(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6261156d-f7da-4146-9964-086cebf28122",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_training_config(use_augmentation, initial_lr):\n",
    "    \"\"\"Print training configuration details\"\"\"\n",
    "    print(\"\\n=== Training Configuration ===\")\n",
    "    print(f\"Initial Learning Rate: {initial_lr}\")\n",
    "    print(f\"Optimizer: Adam (betas=(0.9, 0.999), eps=1e-08)\")\n",
    "    print(f\"Learning Rate Scheduler: ReduceLROnPlateau\")\n",
    "    print(f\" - mode: max (tracking validation accuracy)\")\n",
    "    print(f\" - factor: 0.1\")\n",
    "    print(f\" - patience: 3 epochs\")\n",
    "    print(f\" - min_lr: 1e-6\")\n",
    "    \n",
    "    print(\"\\n=== Data Augmentation Settings ===\")\n",
    "    if use_augmentation:\n",
    "        print(\"Data Augmentation: Enabled for training\")\n",
    "        print(\" - Random rotation: ±10 degrees\")\n",
    "        print(\" - Random zoom: ±10%\")\n",
    "        print(\" - Random shift: ±10% horizontal and vertical\")\n",
    "    else:\n",
    "        print(\"Data Augmentation: Disabled\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f0e16f28-e3a9-4148-8bfe-cbe6ca3cc819",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, train_loader, optimizer, criterion, device, scheduler):\n",
    "    \"\"\"Train for one epoch\"\"\"\n",
    "    model.train()\n",
    "    train_correct = 0\n",
    "    train_total = 0\n",
    "    train_loss = 0.0\n",
    "\n",
    "    train_pbar = tqdm(train_loader, desc=f'Training (lr={scheduler.get_last_lr()[0]:.6f})', leave=False)\n",
    "    for batch_idx, (data, target) in enumerate(train_pbar):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.item()\n",
    "        _, predicted = torch.max(output.data, 1)\n",
    "        train_total += target.size(0)\n",
    "        train_correct += (predicted == target).sum().item()\n",
    "        \n",
    "        train_pbar.set_postfix({\n",
    "            'loss': f'{train_loss/(batch_idx+1):.4f}',\n",
    "            'acc': f'{100.*train_correct/train_total:.2f}%',\n",
    "            'lr': f'{scheduler.get_last_lr()[0]:.6f}'\n",
    "        })\n",
    "    \n",
    "    return train_loss/len(train_loader), 100 * train_correct / train_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "04687b06-fa03-41c1-8cf3-2425892ce152",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, test_loader, criterion, device):\n",
    "    \"\"\"Test the model\"\"\"\n",
    "    model.eval()\n",
    "    test_correct = 0\n",
    "    test_total = 0\n",
    "    test_loss = 0.0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        test_pbar = tqdm(test_loader, desc='Testing')\n",
    "        for batch_idx, (data, target) in enumerate(test_pbar):\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "            test_loss += loss.item()\n",
    "            \n",
    "            _, predicted = torch.max(output.data, 1)\n",
    "            test_total += target.size(0)\n",
    "            test_correct += (predicted == target).sum().item()\n",
    "            \n",
    "            test_pbar.set_postfix({\n",
    "                'loss': f'{test_loss/(batch_idx+1):.4f}',\n",
    "                'acc': f'{100.*test_correct/test_total:.2f}%'\n",
    "            })\n",
    "    \n",
    "    return test_loss/len(test_loader), 100 * test_correct / test_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6c10b868-48f4-47af-840e-7bcd7dc8e1db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(model, val_loader, criterion, device):\n",
    "    \"\"\"Validate the model\"\"\"\n",
    "    model.eval()\n",
    "    val_correct = 0\n",
    "    val_total = 0\n",
    "    val_loss = 0.0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        val_pbar = tqdm(val_loader, desc='Validation', leave=False)\n",
    "        for batch_idx, (data, target) in enumerate(val_pbar):\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "            val_loss += loss.item()\n",
    "            \n",
    "            _, predicted = torch.max(output.data, 1)\n",
    "            val_total += target.size(0)\n",
    "            val_correct += (predicted == target).sum().item()\n",
    "            \n",
    "            val_pbar.set_postfix({\n",
    "                'loss': f'{val_loss/(batch_idx+1):.4f}',\n",
    "                'acc': f'{100.*val_correct/val_total:.2f}%'\n",
    "            })\n",
    "    \n",
    "    return val_loss/len(val_loader), 100 * val_correct / val_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f955a85f-a443-435a-a2a5-6e0540d592c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lr(epoch, initial_lr=0.003):\n",
    "    \"\"\"Custom learning rate scheduler\"\"\"\n",
    "    return round(initial_lr * 1/(1 + 0.319 * epoch), 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ee1e99d4-e6aa-4147-a760-2d9d59c61ffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(use_augmentation=True):\n",
    "    \"\"\"Main training function\"\"\"\n",
    "    BATCH_SIZE = 128\n",
    "    initial_lr = 0.005  # Initial learning rate, 0.003 for SGD\n",
    "    \n",
    "    print(\"\\n=== Initializing Training Pipeline ===\")\n",
    "    setup_directories()\n",
    "    device = setup_device()\n",
    "    \n",
    "    print(\"\\n=== Preparing Data ===\")\n",
    "    train_loader, val_loader, test_loader, train_size, val_size, test_size = load_data(\n",
    "        use_augmentation, BATCH_SIZE\n",
    "    )\n",
    "    \n",
    "    # print(\"\\n=== Dataset Statistics ===\")\n",
    "    # print(f\"Training samples: {train_size}\")\n",
    "    # print(f\"Validation samples: {val_size}\")\n",
    "    # print(f\"Test samples: {test_size}\")\n",
    "    \n",
    "    print_training_config(use_augmentation, initial_lr)\n",
    "    \n",
    "    model = get_model().to(device)\n",
    "    #optimizer = optim.SGD(model.parameters(), lr=initial_lr, momentum=0.9)\n",
    "    optimizer = optim.Adam(\n",
    "        model.parameters(),\n",
    "        lr=initial_lr,\n",
    "        betas=(0.9, 0.999),  # default Adam parameters\n",
    "        eps=1e-08,           # default numerical stability constant\n",
    "        weight_decay=0       # L2 penalty (if needed)\n",
    "    )\n",
    "    criterion = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
    "    \n",
    "    # ReduceLROnPlateau scheduler\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer,\n",
    "        mode='max',           # Since we're tracking validation accuracy\n",
    "        factor=0.1,          # Reduce LR by factor of 10\n",
    "        patience=3,          # Number of epochs with no improvement after which LR will be reduced\n",
    "        #verbose=True,        # Print message when LR is reduced\n",
    "        min_lr=1e-6,        # Minimum LR\n",
    "        threshold=0.001,     # Minimum change to qualify as an improvement\n",
    "        threshold_mode='rel' # Relative change\n",
    "    )\n",
    "    \n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    print(f\"\\nTotal parameters: {total_params:,}\")\n",
    "\n",
    "    summary(model, input_size=(1, 28, 28))\n",
    "    \n",
    "    print(\"\\n=== Starting Training ===\")\n",
    "    best_accuracy = 0.0\n",
    "    \n",
    "    for epoch in range(20):\n",
    "        current_lr = optimizer.param_groups[0]['lr']\n",
    "        print(f\"\\nEpoch Summary:\")\n",
    "        print(f\"Epoch {epoch+1}/20 (LR={current_lr:.6f}):\")\n",
    "        \n",
    "        # Create a simple scheduler wrapper for train_epoch function\n",
    "        class SimpleScheduler:\n",
    "            def get_last_lr(self):\n",
    "                return [current_lr]\n",
    "        \n",
    "        temp_scheduler = SimpleScheduler()\n",
    "        \n",
    "        train_loss, train_accuracy = train_epoch(model, train_loader, optimizer, criterion, device, temp_scheduler)\n",
    "        val_loss, val_accuracy = validate(model, val_loader, criterion, device)\n",
    "        \n",
    "        print(f\"Training Loss: {train_loss:.4f}, Training Accuracy: {train_accuracy:.2f}% | \"\n",
    "              f\"Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_accuracy:.2f}%\")\n",
    "        \n",
    "        # Step the scheduler with validation accuracy\n",
    "        scheduler.step(val_accuracy)\n",
    "        \n",
    "        if val_accuracy > best_accuracy:\n",
    "            best_accuracy = val_accuracy\n",
    "            model_filename = save_model(model, val_accuracy)\n",
    "            print(f\"✓ New best model saved as {model_filename}\")\n",
    "        \n",
    "        if val_accuracy >= 99.4:\n",
    "            print(\"\\n🎉 Reached target validation accuracy of 99.4%!\")\n",
    "            break\n",
    "    \n",
    "    print(\"\\n=== Final Evaluation ===\")\n",
    "    test_loss, test_accuracy = test(model, test_loader, criterion, device)\n",
    "    print(f\"\\nFinal Test Results:\")\n",
    "    print(f\"Test Loss: {test_loss:.4f}\")\n",
    "    print(f\"Test Accuracy: {test_accuracy:.2f}%\")\n",
    "    print(\"\\n=== Training Complete ===\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "034a9460-4195-45e7-9465-1db425cbb4d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Initializing Training Pipeline ===\n",
      "Using device: cuda\n",
      "\n",
      "=== Preparing Data ===\n",
      "Split details:\n",
      "- Total dataset size: 60000\n",
      "- Training set: 50000 samples (randomly selected)\n",
      "- Validation set: 10000 samples (randomly selected)\n",
      "\n",
      "=== Training Configuration ===\n",
      "Initial Learning Rate: 0.0005\n",
      "Optimizer: Adam (betas=(0.9, 0.999), eps=1e-08)\n",
      "Learning Rate Scheduler: ReduceLROnPlateau\n",
      " - mode: max (tracking validation accuracy)\n",
      " - factor: 0.1\n",
      " - patience: 3 epochs\n",
      " - min_lr: 1e-6\n",
      "\n",
      "=== Data Augmentation Settings ===\n",
      "Data Augmentation: Disabled\n",
      "\n",
      "Total parameters: 10,624\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 10, 26, 26]             100\n",
      "       BatchNorm2d-2           [-1, 10, 26, 26]              20\n",
      "           Dropout-3           [-1, 10, 26, 26]               0\n",
      "            Conv2d-4           [-1, 16, 24, 24]           1,456\n",
      "       BatchNorm2d-5           [-1, 16, 24, 24]              32\n",
      "           Dropout-6           [-1, 16, 24, 24]               0\n",
      "         MaxPool2d-7           [-1, 16, 12, 12]               0\n",
      "            Conv2d-8           [-1, 16, 10, 10]           2,320\n",
      "       BatchNorm2d-9           [-1, 16, 10, 10]              32\n",
      "          Dropout-10           [-1, 16, 10, 10]               0\n",
      "           Conv2d-11             [-1, 32, 8, 8]           4,640\n",
      "      BatchNorm2d-12             [-1, 32, 8, 8]              64\n",
      "          Dropout-13             [-1, 32, 8, 8]               0\n",
      "        MaxPool2d-14             [-1, 32, 4, 4]               0\n",
      "           Conv2d-15             [-1, 10, 4, 4]             330\n",
      "           Conv2d-16             [-1, 10, 1, 1]           1,610\n",
      "      BatchNorm2d-17             [-1, 10, 1, 1]              20\n",
      "================================================================\n",
      "Total params: 10,624\n",
      "Trainable params: 10,624\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.47\n",
      "Params size (MB): 0.04\n",
      "Estimated Total Size (MB): 0.52\n",
      "----------------------------------------------------------------\n",
      "\n",
      "=== Starting Training ===\n",
      "\n",
      "Epoch Summary:\n",
      "Epoch 1/20 (LR=0.000500):\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 1.0501, Training Accuracy: 86.79% | Validation Loss: 0.9164, Validation Accuracy: 95.24%\n",
      "✓ New best model saved as mnist_model_95.24_20241129_205818.pth\n",
      "\n",
      "Epoch Summary:\n",
      "Epoch 2/20 (LR=0.000500):\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.7342, Training Accuracy: 96.44% | Validation Loss: 0.7599, Validation Accuracy: 96.85%\n",
      "✓ New best model saved as mnist_model_96.85_20241129_205837.pth\n",
      "\n",
      "Epoch Summary:\n",
      "Epoch 3/20 (LR=0.000500):\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.6585, Training Accuracy: 97.39% | Validation Loss: 0.6670, Validation Accuracy: 97.87%\n",
      "✓ New best model saved as mnist_model_97.87_20241129_205855.pth\n",
      "\n",
      "Epoch Summary:\n",
      "Epoch 4/20 (LR=0.000500):\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.6201, Training Accuracy: 97.79% | Validation Loss: 0.6429, Validation Accuracy: 98.19%\n",
      "✓ New best model saved as mnist_model_98.19_20241129_205914.pth\n",
      "\n",
      "Epoch Summary:\n",
      "Epoch 5/20 (LR=0.000500):\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.6011, Training Accuracy: 98.13% | Validation Loss: 0.6030, Validation Accuracy: 98.64%\n",
      "✓ New best model saved as mnist_model_98.64_20241129_205933.pth\n",
      "\n",
      "Epoch Summary:\n",
      "Epoch 6/20 (LR=0.000500):\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.5888, Training Accuracy: 98.31% | Validation Loss: 0.5911, Validation Accuracy: 98.71%\n",
      "✓ New best model saved as mnist_model_98.71_20241129_205952.pth\n",
      "\n",
      "Epoch Summary:\n",
      "Epoch 7/20 (LR=0.000500):\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.5821, Training Accuracy: 98.50% | Validation Loss: 0.5801, Validation Accuracy: 98.88%\n",
      "✓ New best model saved as mnist_model_98.88_20241129_210010.pth\n",
      "\n",
      "Epoch Summary:\n",
      "Epoch 8/20 (LR=0.000500):\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.5758, Training Accuracy: 98.69% | Validation Loss: 0.5751, Validation Accuracy: 98.83%\n",
      "\n",
      "Epoch Summary:\n",
      "Epoch 9/20 (LR=0.000500):\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.5722, Training Accuracy: 98.69% | Validation Loss: 0.5750, Validation Accuracy: 98.87%\n",
      "\n",
      "Epoch Summary:\n",
      "Epoch 10/20 (LR=0.000500):\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.5697, Training Accuracy: 98.80% | Validation Loss: 0.5655, Validation Accuracy: 98.98%\n",
      "✓ New best model saved as mnist_model_98.98_20241129_210106.pth\n",
      "\n",
      "Epoch Summary:\n",
      "Epoch 11/20 (LR=0.000500):\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.5686, Training Accuracy: 98.82% | Validation Loss: 0.5624, Validation Accuracy: 98.96%\n",
      "\n",
      "Epoch Summary:\n",
      "Epoch 12/20 (LR=0.000500):\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.5658, Training Accuracy: 98.86% | Validation Loss: 0.5558, Validation Accuracy: 99.08%\n",
      "✓ New best model saved as mnist_model_99.08_20241129_210144.pth\n",
      "\n",
      "Epoch Summary:\n",
      "Epoch 13/20 (LR=0.000500):\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.5643, Training Accuracy: 98.93% | Validation Loss: 0.5583, Validation Accuracy: 99.04%\n",
      "\n",
      "Epoch Summary:\n",
      "Epoch 14/20 (LR=0.000500):\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.5633, Training Accuracy: 98.96% | Validation Loss: 0.5530, Validation Accuracy: 99.16%\n",
      "✓ New best model saved as mnist_model_99.16_20241129_210221.pth\n",
      "\n",
      "Epoch Summary:\n",
      "Epoch 15/20 (LR=0.000500):\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.5614, Training Accuracy: 99.00% | Validation Loss: 0.5573, Validation Accuracy: 99.16%\n",
      "\n",
      "Epoch Summary:\n",
      "Epoch 16/20 (LR=0.000500):\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.5617, Training Accuracy: 98.98% | Validation Loss: 0.5493, Validation Accuracy: 99.25%\n",
      "✓ New best model saved as mnist_model_99.25_20241129_210258.pth\n",
      "\n",
      "Epoch Summary:\n",
      "Epoch 17/20 (LR=0.000500):\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.5599, Training Accuracy: 99.11% | Validation Loss: 0.5511, Validation Accuracy: 99.18%\n",
      "\n",
      "Epoch Summary:\n",
      "Epoch 18/20 (LR=0.000500):\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.5590, Training Accuracy: 99.07% | Validation Loss: 0.5521, Validation Accuracy: 99.09%\n",
      "\n",
      "Epoch Summary:\n",
      "Epoch 19/20 (LR=0.000500):\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.5571, Training Accuracy: 99.07% | Validation Loss: 0.5536, Validation Accuracy: 99.13%\n",
      "\n",
      "Epoch Summary:\n",
      "Epoch 20/20 (LR=0.000500):\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.5564, Training Accuracy: 99.15% | Validation Loss: 0.5508, Validation Accuracy: 99.22%\n",
      "\n",
      "=== Final Evaluation ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|████████████████████████████████████████████████| 79/79 [00:02<00:00, 30.63it/s, loss=0.5469, acc=99.24%]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final Test Results:\n",
      "Test Loss: 0.5469\n",
      "Test Accuracy: 99.24%\n",
      "\n",
      "=== Training Complete ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Pass appropriate flags if data augmentation is required or not\n",
    "# True  - Data augmentation to be applied\n",
    "# False - No Data augmentation\n",
    "\n",
    "# train(use_augmentation=True)\n",
    "train(use_augmentation=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dcd00d1-a8d7-450c-b4bd-d998a8066a85",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49dae42e-7c2c-49c0-80bc-444711b8e416",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pytorch 2.5 (pytorch)",
   "language": "python",
   "name": "pytorch25"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
