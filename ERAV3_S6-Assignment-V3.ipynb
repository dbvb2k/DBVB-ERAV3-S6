{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8ff99711-ebe0-4ca2-a16d-938db09e15db",
   "metadata": {},
   "source": [
    "### Necessary imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2ecdaae1-bd11-4a74-9572-e655a778a3e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import random_split, Subset\n",
    "from torchsummary import summary\n",
    "from S6_Model import get_model, save_model\n",
    "#from S6_Model3 import get_model, save_model\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import argparse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72ca6239-1a09-476d-9f47-2b85742f1cf3",
   "metadata": {},
   "source": [
    "### Data Augmentation and Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8a2be9f1-d52e-4d58-abb0-03f2e85fb622",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_transform(apply_augmentation=False):\n",
    "    \"\"\"Get data transformation pipeline with enhanced augmentation\"\"\"\n",
    "    transforms_list = [\n",
    "        transforms.ToTensor(),  # Convert to tensor first\n",
    "        transforms.Normalize((0.1307,), (0.3081,))\n",
    "    ]\n",
    "    \n",
    "    if apply_augmentation:\n",
    "        transforms_list = [\n",
    "            transforms.RandomAffine(\n",
    "                degrees=10,\n",
    "                translate=(0.1, 0.1),\n",
    "                scale=(0.9, 1.1),\n",
    "                fill=0,\n",
    "            ),\n",
    "            transforms.RandomPerspective(distortion_scale=0.2, p=0.5),\n",
    "        ] + transforms_list  # Add base transforms after augmentation\n",
    "        \n",
    "        # Add RandomErasing after converting to tensor\n",
    "        transforms_list.append(transforms.RandomErasing(p=0.1))\n",
    "    \n",
    "    return transforms.Compose(transforms_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49dcb7ad-33e6-447e-b790-df958f29f745",
   "metadata": {},
   "source": [
    "### Create the 'models' directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1df6bb2f-de76-47a7-90ba-15abc0b67fc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_directories():\n",
    "    \"\"\"Create necessary directories\"\"\"\n",
    "    if not os.path.exists('models'):\n",
    "        os.makedirs('models')\n",
    "        print(\"Created 'models' directory for saving checkpoints\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbafcfb0-e480-4ccc-b0ce-09ab6deb71b9",
   "metadata": {},
   "source": [
    "### Use CUDA if available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0df7161a-6a1c-4356-beb6-6cd392369ea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_device():\n",
    "    \"\"\"Set up and return the device to use\"\"\"\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "    return device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8535548-f625-4cd0-b7ac-314706733af9",
   "metadata": {},
   "source": [
    "### Set the seed value for reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "21591f1d-01da-40df-a800-41ef8943370b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add this at the start of your notebook or training script\n",
    "def set_seed(seed=42):\n",
    "    \"\"\"Set seeds for reproducibility\"\"\"\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c587b49b-d0c1-456f-b0ef-83935d97ecf5",
   "metadata": {},
   "source": [
    "### Load the data, Training set = 50k, Test/Val set = 10k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "276f917b-12e0-4225-8692-e5415f7cd48d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use it before loading data\n",
    "set_seed(42)\n",
    "\n",
    "def load_data(use_augmentation, batch_size=128):\n",
    "    \"\"\"Load and prepare data loaders with shuffled split\"\"\"\n",
    "    train_transform = get_transform(apply_augmentation=use_augmentation)\n",
    "    test_transform = get_transform(apply_augmentation=False)\n",
    "    \n",
    "    # Load full training dataset\n",
    "    full_train_dataset = datasets.MNIST('./data', train=True, download=True, transform=train_transform)\n",
    "    test_dataset = datasets.MNIST('./data', train=False, transform=test_transform)\n",
    "    \n",
    "    # Generate shuffled indices\n",
    "    num_train = len(full_train_dataset)\n",
    "    indices = torch.randperm(num_train)  # Creates a random permutation of indices\n",
    "    \n",
    "    # Split indices\n",
    "    train_size = 50000\n",
    "    train_indices = indices[:train_size]\n",
    "    val_indices = indices[train_size:]\n",
    "    \n",
    "    print(f\"Split details:\")\n",
    "    print(f\"- Total dataset size: {num_train}\")\n",
    "    print(f\"- Training set: {len(train_indices)} samples (randomly selected)\")\n",
    "    print(f\"- Validation set: {len(val_indices)} samples (randomly selected)\")\n",
    "    \n",
    "    # Create subsets using shuffled indices\n",
    "    train_dataset = Subset(full_train_dataset, train_indices)\n",
    "    val_dataset = Subset(\n",
    "        datasets.MNIST('./data', train=True, download=False, transform=test_transform),\n",
    "        val_indices\n",
    "    )\n",
    "    \n",
    "    # Create data loaders\n",
    "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)  # Additional shuffling during training\n",
    "    val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, shuffle=False)     # No need to shuffle validation set\n",
    "    test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    return train_loader, val_loader, test_loader, len(train_dataset), len(val_dataset), len(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6261156d-f7da-4146-9964-086cebf28122",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_training_config(use_augmentation, initial_lr):\n",
    "    \"\"\"Print training configuration details\"\"\"\n",
    "    print(\"\\n===== Training Configuration =====\")\n",
    "    print(f\"Initial Learning Rate: {initial_lr}\")\n",
    "    print(f\"Optimizer: Adam (betas=(0.9, 0.999), eps=1e-08)\")\n",
    "    print(f\"Learning Rate Scheduler: Simple custom LR scheduler\")\n",
    "    # print(f\"Learning Rate Scheduler: ReduceLROnPlateau\")\n",
    "    # print(f\" - mode: max (tracking validation accuracy)\")\n",
    "    # print(f\" - factor: 0.1\")\n",
    "    # print(f\" - patience: 3 epochs\")\n",
    "    # print(f\" - min_lr: 1e-6\")\n",
    "    \n",
    "    print(\"\\n===== Data Augmentation Settings =====\")\n",
    "    if use_augmentation:\n",
    "        print(\"Data Augmentation: Enabled for training\")\n",
    "        print(\" - Random rotation: Â±10 degrees\")\n",
    "        print(\" - Random zoom: Â±10%\")\n",
    "        print(\" - Random shift: Â±10% horizontal and vertical\")\n",
    "    else:\n",
    "        print(\"Data Augmentation: Disabled\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ec271fb-a98a-499b-bab5-b371ed304e5f",
   "metadata": {},
   "source": [
    "### Train one epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f0e16f28-e3a9-4148-8bfe-cbe6ca3cc819",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, train_loader, optimizer, criterion, device, scheduler):\n",
    "    \"\"\"Train for one epoch\"\"\"\n",
    "    model.train()\n",
    "    train_correct = 0\n",
    "    train_total = 0\n",
    "    train_loss = 0.0\n",
    "\n",
    "    train_pbar = tqdm(train_loader, desc=f'Training (lr={scheduler.get_last_lr()[0]:.6f})', leave=False)\n",
    "    for batch_idx, (data, target) in enumerate(train_pbar):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.item()\n",
    "        _, predicted = torch.max(output.data, 1)\n",
    "        train_total += target.size(0)\n",
    "        train_correct += (predicted == target).sum().item()\n",
    "        \n",
    "        train_pbar.set_postfix({\n",
    "            'loss': f'{train_loss/(batch_idx+1):.4f}',\n",
    "            'acc': f'{100.*train_correct/train_total:.2f}%',\n",
    "            'lr': f'{scheduler.get_last_lr()[0]:.6f}'\n",
    "        })\n",
    "    \n",
    "    return train_loss/len(train_loader), 100 * train_correct / train_total"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a49244bb-4640-47d4-8b36-2e5d8c3498ec",
   "metadata": {},
   "source": [
    "### Test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "04687b06-fa03-41c1-8cf3-2425892ce152",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, test_loader, criterion, device):\n",
    "    \"\"\"Test the model\"\"\"\n",
    "    model.eval()\n",
    "    test_correct = 0\n",
    "    test_total = 0\n",
    "    test_loss = 0.0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        test_pbar = tqdm(test_loader, desc='Testing')\n",
    "        for batch_idx, (data, target) in enumerate(test_pbar):\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "            test_loss += loss.item()\n",
    "            \n",
    "            _, predicted = torch.max(output.data, 1)\n",
    "            test_total += target.size(0)\n",
    "            test_correct += (predicted == target).sum().item()\n",
    "            \n",
    "            test_pbar.set_postfix({\n",
    "                'loss': f'{test_loss/(batch_idx+1):.4f}',\n",
    "                'acc': f'{100.*test_correct/test_total:.2f}%'\n",
    "            })\n",
    "    \n",
    "    return test_loss/len(test_loader), 100 * test_correct / test_total"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c24c4f19-0067-4b7e-87c8-73943c144b8f",
   "metadata": {},
   "source": [
    "### Validate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6c10b868-48f4-47af-840e-7bcd7dc8e1db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(model, val_loader, criterion, device):\n",
    "    \"\"\"Validate the model\"\"\"\n",
    "    model.eval()\n",
    "    val_correct = 0\n",
    "    val_total = 0\n",
    "    val_loss = 0.0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        val_pbar = tqdm(val_loader, desc='Validation', leave=False)\n",
    "        for batch_idx, (data, target) in enumerate(val_pbar):\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "            val_loss += loss.item()\n",
    "            \n",
    "            _, predicted = torch.max(output.data, 1)\n",
    "            val_total += target.size(0)\n",
    "            val_correct += (predicted == target).sum().item()\n",
    "            \n",
    "            val_pbar.set_postfix({\n",
    "                'loss': f'{val_loss/(batch_idx+1):.4f}',\n",
    "                'acc': f'{100.*val_correct/val_total:.2f}%'\n",
    "            })\n",
    "    \n",
    "    return val_loss/len(val_loader), 100 * val_correct / val_total"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "239a5245-671f-409a-a18f-b0e24f21aac2",
   "metadata": {},
   "source": [
    "### Custom LR scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f955a85f-a443-435a-a2a5-6e0540d592c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lr_schedule(epoch):\n",
    "    \"\"\"Custom learning rate scheduler\"\"\"    \n",
    "    lr_dict = {\n",
    "        16: 0.00025,\n",
    "        10: 0.0005,\n",
    "        6: 0.001,\n",
    "        4: 0.002,\n",
    "        0: 0.003\n",
    "    }\n",
    "    \n",
    "    for threshold, lr in sorted(lr_dict.items(), reverse=True):\n",
    "        if epoch >= threshold:\n",
    "            return lr\n",
    "    return lr_dict[0]\n",
    "    # return round(initial_lr * 1/(1 + 0.319 * epoch), 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8884e912-42f2-42e6-ae95-0ca1cb55a1db",
   "metadata": {},
   "source": [
    "### Main train function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "97ecd2c9-cdf0-40ca-827f-1790cff768ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(use_augmentation=True):\n",
    "    \"\"\"Main training function\"\"\"\n",
    "    BATCH_SIZE = 128\n",
    "    initial_lr = 0.003  # Initial learning rate\n",
    "    \n",
    "    print(\"\\n=== Initializing Training Pipeline ===\")\n",
    "    setup_directories()\n",
    "    device = setup_device()\n",
    "    \n",
    "    print(\"\\n=== Preparing Data ===\")\n",
    "    train_loader, val_loader, test_loader, train_size, val_size, test_size = load_data(\n",
    "        use_augmentation, BATCH_SIZE\n",
    "    )\n",
    "    \n",
    "    print(\"\\n=== Dataset Statistics ===\")\n",
    "    print(f\"Training samples: {train_size}\")\n",
    "    print(f\"Validation samples: {val_size}\")\n",
    "    print(f\"Test samples: {test_size}\")\n",
    "    \n",
    "    model = get_model().to(device)\n",
    "    optimizer = optim.Adam(\n",
    "        model.parameters(),\n",
    "        lr=initial_lr,\n",
    "        betas=(0.9, 0.999),\n",
    "        eps=1e-08\n",
    "    )\n",
    "    \n",
    "    # Initialize ReduceLROnPlateau scheduler\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer,\n",
    "        mode='max',           # Since we're tracking validation accuracy\n",
    "        factor=0.1,           # Reduce LR by factor of 10\n",
    "        patience=3,           # Number of epochs with no improvement after which LR will be reduced\n",
    "        # verbose=True,         # Print message when LR is reduced\n",
    "        min_lr=1e-6,          # Minimum LR\n",
    "        threshold=0.001,      # Minimum change to qualify as an improvement\n",
    "        threshold_mode='rel'  # Relative change\n",
    "    )\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
    "    \n",
    "    print(\"\\n=== Training Configuration ===\")\n",
    "    print(f\"Initial Learning Rate: {initial_lr}\")\n",
    "    print(f\"Optimizer: Adam (betas=(0.9, 0.999), eps=1e-08)\")\n",
    "    print(f\"Scheduler: ReduceLROnPlateau\")\n",
    "    print(f\" - mode: max (tracking validation accuracy)\")\n",
    "    print(f\" - factor: 0.1\")\n",
    "    print(f\" - patience: 3 epochs\")\n",
    "    print(f\" - min_lr: 1e-6\")\n",
    "    print(f\" - threshold: 0.001\")\n",
    "    \n",
    "    # total_params = sum(p.numel() for p in model.parameters())\n",
    "    # print(f\"Total parameters: {total_params:,}\")\n",
    "    print(\"\\nModel Summary: \")\n",
    "    summary(model, input_size=(1, 28, 28))\n",
    "\n",
    "    print(\"\\n=== Starting Training ===\")\n",
    "    best_accuracy = 0.0\n",
    "    \n",
    "    for epoch in range(20):\n",
    "        current_lr = optimizer.param_groups[0]['lr']\n",
    "        print(f\"\\nEpoch {epoch+1}/20 (LR={current_lr:.6f}):\")\n",
    "        \n",
    "        # Create a simple scheduler wrapper for train_epoch function\n",
    "        class SimpleScheduler:\n",
    "            def get_last_lr(self):\n",
    "                return [current_lr]\n",
    "        \n",
    "        temp_scheduler = SimpleScheduler()\n",
    "        \n",
    "        train_loss, train_accuracy = train_epoch(model, train_loader, optimizer, criterion, device, temp_scheduler)\n",
    "        val_loss, val_accuracy = validate(model, val_loader, criterion, device)\n",
    "        \n",
    "        print(f\"\\nEpoch Summary:\")\n",
    "        print(f\"Training Loss: {train_loss:.4f}, Training Accuracy: {train_accuracy:.2f}% | \"\n",
    "              f\"Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_accuracy:.2f}%\")\n",
    "        print(f\"Learning Rate: {current_lr:.6f}\")\n",
    "        \n",
    "        # Step the scheduler with validation accuracy\n",
    "        scheduler.step(val_accuracy)\n",
    "        \n",
    "        if val_accuracy > best_accuracy:\n",
    "            best_accuracy = val_accuracy\n",
    "            model_filename = save_model(model, val_accuracy)\n",
    "            print(f\"âœ“ New best model saved as {model_filename}\")\n",
    "            print(f\"Previous best: {best_accuracy:.2f}%\")\n",
    "        \n",
    "        # Print plateau detection info\n",
    "        if epoch > scheduler.patience:\n",
    "            recent_vals = [val_accuracy]  # You might want to keep track of previous accuracies\n",
    "            max_recent = max(recent_vals)\n",
    "            print(f\"\\nPlateau Monitor:\")\n",
    "            print(f\"Recent accuracy: {val_accuracy:.2f}% | \"\n",
    "                  f\"Best recent: {max_recent:.2f}% | \"\n",
    "                  f\"Improvement needed: >{max_recent + scheduler.threshold*max_recent:.2f}%\")\n",
    "        \n",
    "        if val_accuracy >= 99.4:\n",
    "            print(\"\\nðŸŽ‰ Reached target validation accuracy of 99.4%!\")\n",
    "            break\n",
    "    \n",
    "    print(\"\\n=== Final Evaluation ===\")\n",
    "    test_loss, test_accuracy = test(model, test_loader, criterion, device)\n",
    "    print(f\"\\nFinal Test Results:\")\n",
    "    print(f\"Test Loss: {test_loss:.4f}\")\n",
    "    print(f\"Test Accuracy: {test_accuracy:.2f}%\")\n",
    "    print(\"\\n=== Training Complete ===\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "034a9460-4195-45e7-9465-1db425cbb4d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Initializing Training Pipeline ===\n",
      "Using device: cuda\n",
      "\n",
      "=== Preparing Data ===\n",
      "Split details:\n",
      "- Total dataset size: 60000\n",
      "- Training set: 50000 samples (randomly selected)\n",
      "- Validation set: 10000 samples (randomly selected)\n",
      "\n",
      "=== Dataset Statistics ===\n",
      "Training samples: 50000\n",
      "Validation samples: 10000\n",
      "Test samples: 10000\n",
      "\n",
      "=== Training Configuration ===\n",
      "Initial Learning Rate: 0.003\n",
      "Optimizer: Adam (betas=(0.9, 0.999), eps=1e-08)\n",
      "Scheduler: ReduceLROnPlateau\n",
      " - mode: max (tracking validation accuracy)\n",
      " - factor: 0.1\n",
      " - patience: 3 epochs\n",
      " - min_lr: 1e-6\n",
      " - threshold: 0.001\n",
      "\n",
      "Model Summary: \n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1            [-1, 8, 26, 26]              80\n",
      "            Conv2d-2           [-1, 16, 24, 24]           1,168\n",
      "       BatchNorm2d-3           [-1, 16, 24, 24]              32\n",
      "           Dropout-4           [-1, 16, 24, 24]               0\n",
      "         MaxPool2d-5           [-1, 16, 12, 12]               0\n",
      "            Conv2d-6           [-1, 16, 10, 10]           2,320\n",
      "       BatchNorm2d-7           [-1, 16, 10, 10]              32\n",
      "           Dropout-8           [-1, 16, 10, 10]               0\n",
      "            Conv2d-9             [-1, 16, 8, 8]           2,320\n",
      "      BatchNorm2d-10             [-1, 16, 8, 8]              32\n",
      "          Dropout-11             [-1, 16, 8, 8]               0\n",
      "        MaxPool2d-12             [-1, 16, 4, 4]               0\n",
      "           Conv2d-13             [-1, 16, 2, 2]           2,320\n",
      "           Conv2d-14             [-1, 10, 1, 1]             650\n",
      "AdaptiveAvgPool2d-15             [-1, 10, 1, 1]               0\n",
      "================================================================\n",
      "Total params: 8,954\n",
      "Trainable params: 8,954\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.33\n",
      "Params size (MB): 0.03\n",
      "Estimated Total Size (MB): 0.37\n",
      "----------------------------------------------------------------\n",
      "\n",
      "=== Starting Training ===\n",
      "\n",
      "Epoch 1/20 (LR=0.003000):\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch Summary:\n",
      "Training Loss: 0.8810, Training Accuracy: 86.84% | Validation Loss: 0.6117, Validation Accuracy: 97.88%\n",
      "Learning Rate: 0.003000\n",
      "âœ“ New best model saved as mnist_model_97.88_20241201_165445.pth\n",
      "Previous best: 97.88%\n",
      "\n",
      "Epoch 2/20 (LR=0.003000):\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch Summary:\n",
      "Training Loss: 0.6777, Training Accuracy: 95.03% | Validation Loss: 0.5914, Validation Accuracy: 98.22%\n",
      "Learning Rate: 0.003000\n",
      "âœ“ New best model saved as mnist_model_98.22_20241201_165526.pth\n",
      "Previous best: 98.22%\n",
      "\n",
      "Epoch 3/20 (LR=0.003000):\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training (lr=0.003000):  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                  | 257/391 [00:25<00:13,  9.83it/s, loss=0.6544, acc=95.76%, lr=0.003000]"
     ]
    }
   ],
   "source": [
    "# Pass appropriate flags if data augmentation is required or not\n",
    "# True  - Data augmentation to be applied\n",
    "# False - No Data augmentation\n",
    "\n",
    "train(use_augmentation=True)\n",
    "#train(use_augmentation=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dcd00d1-a8d7-450c-b4bd-d998a8066a85",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49dae42e-7c2c-49c0-80bc-444711b8e416",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pytorch 2.5 (pytorch)",
   "language": "python",
   "name": "pytorch25"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
