{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2ecdaae1-bd11-4a74-9572-e655a778a3e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import random_split, Subset\n",
    "# from S6_model import get_model, save_model\n",
    "from Newmodel import get_model, save_model\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import argparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8a2be9f1-d52e-4d58-abb0-03f2e85fb622",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_transform(apply_augmentation=False):\n",
    "    \"\"\"Get data transformation pipeline with enhanced augmentation\"\"\"\n",
    "    transforms_list = [\n",
    "        transforms.ToTensor(),  # Convert to tensor first\n",
    "        transforms.Normalize((0.1307,), (0.3081,))\n",
    "    ]\n",
    "    \n",
    "    if apply_augmentation:\n",
    "        transforms_list = [\n",
    "            transforms.RandomAffine(\n",
    "                degrees=10,\n",
    "                translate=(0.1, 0.1),\n",
    "                scale=(0.9, 1.1),\n",
    "                fill=0,\n",
    "            ),\n",
    "            transforms.RandomPerspective(distortion_scale=0.2, p=0.5),\n",
    "        ] + transforms_list  # Add base transforms after augmentation\n",
    "        \n",
    "        # Add RandomErasing after converting to tensor\n",
    "        transforms_list.append(transforms.RandomErasing(p=0.1))\n",
    "    \n",
    "    return transforms.Compose(transforms_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1df6bb2f-de76-47a7-90ba-15abc0b67fc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_directories():\n",
    "    \"\"\"Create necessary directories\"\"\"\n",
    "    if not os.path.exists('models'):\n",
    "        os.makedirs('models')\n",
    "        print(\"Created 'models' directory for saving checkpoints\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0df7161a-6a1c-4356-beb6-6cd392369ea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_device():\n",
    "    \"\"\"Set up and return the device to use\"\"\"\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "    return device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "276f917b-12e0-4225-8692-e5415f7cd48d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(use_augmentation, batch_size=128):\n",
    "    \"\"\"Load and prepare data loaders\"\"\"\n",
    "    train_transform = get_transform(apply_augmentation=use_augmentation)\n",
    "    test_transform = get_transform(apply_augmentation=False)\n",
    "    \n",
    "    full_train_dataset = datasets.MNIST('./data', train=True, download=True, transform=train_transform)\n",
    "    test_dataset = datasets.MNIST('./data', train=False, transform=test_transform)\n",
    "    \n",
    "    train_indices = list(range(50000))\n",
    "    val_indices = list(range(50000, 60000))\n",
    "    \n",
    "    train_dataset = Subset(full_train_dataset, train_indices)\n",
    "    val_dataset = Subset(\n",
    "        datasets.MNIST('./data', train=True, download=False, transform=test_transform),\n",
    "        val_indices\n",
    "    )\n",
    "    \n",
    "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "    test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    return train_loader, val_loader, test_loader, len(train_dataset), len(val_dataset), len(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6261156d-f7da-4146-9964-086cebf28122",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_training_config(use_augmentation, initial_lr):\n",
    "    \"\"\"Print training configuration details\"\"\"\n",
    "    print(\"\\n=== Training Configuration ===\")\n",
    "    print(f\"Initial Learning Rate: {initial_lr}\")\n",
    "    print(f\"Optimizer: Adam (betas=(0.9, 0.999), eps=1e-08)\")\n",
    "    print(f\"Learning Rate Scheduler: ReduceLROnPlateau\")\n",
    "    print(f\" - mode: max (tracking validation accuracy)\")\n",
    "    print(f\" - factor: 0.1\")\n",
    "    print(f\" - patience: 3 epochs\")\n",
    "    print(f\" - min_lr: 1e-6\")\n",
    "    \n",
    "    print(\"\\n=== Data Augmentation Settings ===\")\n",
    "    if use_augmentation:\n",
    "        print(\"Data Augmentation: Enabled for training\")\n",
    "        print(\" - Random rotation: ±10 degrees\")\n",
    "        print(\" - Random zoom: ±10%\")\n",
    "        print(\" - Random shift: ±10% horizontal and vertical\")\n",
    "    else:\n",
    "        print(\"Data Augmentation: Disabled\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f0e16f28-e3a9-4148-8bfe-cbe6ca3cc819",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, train_loader, optimizer, criterion, device, scheduler):\n",
    "    \"\"\"Train for one epoch\"\"\"\n",
    "    model.train()\n",
    "    train_correct = 0\n",
    "    train_total = 0\n",
    "    train_loss = 0.0\n",
    "    \n",
    "    train_pbar = tqdm(train_loader, desc=f'Training (lr={scheduler.get_last_lr()[0]:.6f})', leave=False)\n",
    "    for batch_idx, (data, target) in enumerate(train_pbar):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.item()\n",
    "        _, predicted = torch.max(output.data, 1)\n",
    "        train_total += target.size(0)\n",
    "        train_correct += (predicted == target).sum().item()\n",
    "        \n",
    "        train_pbar.set_postfix({\n",
    "            'loss': f'{train_loss/(batch_idx+1):.4f}',\n",
    "            'acc': f'{100.*train_correct/train_total:.2f}%',\n",
    "            'lr': f'{scheduler.get_last_lr()[0]:.6f}'\n",
    "        })\n",
    "    \n",
    "    return train_loss/len(train_loader), 100 * train_correct / train_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "04687b06-fa03-41c1-8cf3-2425892ce152",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, test_loader, criterion, device):\n",
    "    \"\"\"Test the model\"\"\"\n",
    "    model.eval()\n",
    "    test_correct = 0\n",
    "    test_total = 0\n",
    "    test_loss = 0.0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        test_pbar = tqdm(test_loader, desc='Testing')\n",
    "        for batch_idx, (data, target) in enumerate(test_pbar):\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "            test_loss += loss.item()\n",
    "            \n",
    "            _, predicted = torch.max(output.data, 1)\n",
    "            test_total += target.size(0)\n",
    "            test_correct += (predicted == target).sum().item()\n",
    "            \n",
    "            test_pbar.set_postfix({\n",
    "                'loss': f'{test_loss/(batch_idx+1):.4f}',\n",
    "                'acc': f'{100.*test_correct/test_total:.2f}%'\n",
    "            })\n",
    "    \n",
    "    return test_loss/len(test_loader), 100 * test_correct / test_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6c10b868-48f4-47af-840e-7bcd7dc8e1db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(model, val_loader, criterion, device):\n",
    "    \"\"\"Validate the model\"\"\"\n",
    "    model.eval()\n",
    "    val_correct = 0\n",
    "    val_total = 0\n",
    "    val_loss = 0.0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        val_pbar = tqdm(val_loader, desc='Validation', leave=False)\n",
    "        for batch_idx, (data, target) in enumerate(val_pbar):\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "            val_loss += loss.item()\n",
    "            \n",
    "            _, predicted = torch.max(output.data, 1)\n",
    "            val_total += target.size(0)\n",
    "            val_correct += (predicted == target).sum().item()\n",
    "            \n",
    "            val_pbar.set_postfix({\n",
    "                'loss': f'{val_loss/(batch_idx+1):.4f}',\n",
    "                'acc': f'{100.*val_correct/val_total:.2f}%'\n",
    "            })\n",
    "    \n",
    "    return val_loss/len(val_loader), 100 * val_correct / val_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f955a85f-a443-435a-a2a5-6e0540d592c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lr(epoch, initial_lr=0.003):\n",
    "    \"\"\"Custom learning rate scheduler\"\"\"\n",
    "    return round(initial_lr * 1/(1 + 0.319 * epoch), 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ee1e99d4-e6aa-4147-a760-2d9d59c61ffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(use_augmentation=True):\n",
    "    \"\"\"Main training function\"\"\"\n",
    "    BATCH_SIZE = 128\n",
    "    initial_lr = 0.001  # Initial learning rate, 0.003 for SGD\n",
    "    \n",
    "    print(\"\\n=== Initializing Training Pipeline ===\")\n",
    "    setup_directories()\n",
    "    device = setup_device()\n",
    "    \n",
    "    print(\"\\n=== Preparing Data ===\")\n",
    "    train_loader, val_loader, test_loader, train_size, val_size, test_size = load_data(\n",
    "        use_augmentation, BATCH_SIZE\n",
    "    )\n",
    "    \n",
    "    print(\"\\n=== Dataset Statistics ===\")\n",
    "    print(f\"Training samples: {train_size}\")\n",
    "    print(f\"Validation samples: {val_size}\")\n",
    "    print(f\"Test samples: {test_size}\")\n",
    "    \n",
    "    print_training_config(use_augmentation, initial_lr)\n",
    "    \n",
    "    model = get_model().to(device)\n",
    "    #optimizer = optim.SGD(model.parameters(), lr=initial_lr, momentum=0.9)\n",
    "    optimizer = optim.Adam(\n",
    "        model.parameters(),\n",
    "        lr=initial_lr,\n",
    "        betas=(0.9, 0.999),  # default Adam parameters\n",
    "        eps=1e-08,           # default numerical stability constant\n",
    "        weight_decay=0       # L2 penalty (if needed)\n",
    "    )\n",
    "    criterion = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
    "    \n",
    "    # ReduceLROnPlateau scheduler\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer,\n",
    "        mode='max',           # Since we're tracking validation accuracy\n",
    "        factor=0.1,          # Reduce LR by factor of 10\n",
    "        patience=3,          # Number of epochs with no improvement after which LR will be reduced\n",
    "        #verbose=True,        # Print message when LR is reduced\n",
    "        min_lr=1e-6,        # Minimum LR\n",
    "        threshold=0.001,     # Minimum change to qualify as an improvement\n",
    "        threshold_mode='rel' # Relative change\n",
    "    )\n",
    "    \n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    print(f\"Total parameters: {total_params:,}\")\n",
    "    \n",
    "    print(\"\\n=== Starting Training ===\")\n",
    "    best_accuracy = 0.0\n",
    "    \n",
    "    for epoch in range(20):\n",
    "        current_lr = optimizer.param_groups[0]['lr']\n",
    "        print(f\"\\nEpoch Summary:\")\n",
    "        print(f\"Epoch {epoch+1}/20 (LR={current_lr:.6f}):\")\n",
    "        \n",
    "        # Create a simple scheduler wrapper for train_epoch function\n",
    "        class SimpleScheduler:\n",
    "            def get_last_lr(self):\n",
    "                return [current_lr]\n",
    "        \n",
    "        temp_scheduler = SimpleScheduler()\n",
    "        \n",
    "        train_loss, train_accuracy = train_epoch(model, train_loader, optimizer, criterion, device, temp_scheduler)\n",
    "        val_loss, val_accuracy = validate(model, val_loader, criterion, device)\n",
    "        \n",
    "        print(f\"Training Loss: {train_loss:.4f}, Training Accuracy: {train_accuracy:.2f}% | \"\n",
    "              f\"Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_accuracy:.2f}%\")\n",
    "        \n",
    "        # Step the scheduler with validation accuracy\n",
    "        scheduler.step(val_accuracy)\n",
    "        \n",
    "        if val_accuracy > best_accuracy:\n",
    "            best_accuracy = val_accuracy\n",
    "            model_filename = save_model(model, val_accuracy)\n",
    "            print(f\"✓ New best model saved as {model_filename}\")\n",
    "        \n",
    "        if val_accuracy >= 99.4:\n",
    "            print(\"\\n🎉 Reached target validation accuracy of 99.4%!\")\n",
    "            break\n",
    "    \n",
    "    print(\"\\n=== Final Evaluation ===\")\n",
    "    test_loss, test_accuracy = test(model, test_loader, criterion, device)\n",
    "    print(f\"\\nFinal Test Results:\")\n",
    "    print(f\"Test Loss: {test_loss:.4f}\")\n",
    "    print(f\"Test Accuracy: {test_accuracy:.2f}%\")\n",
    "    print(\"\\n=== Training Complete ===\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "034a9460-4195-45e7-9465-1db425cbb4d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Initializing Training Pipeline ===\n",
      "Using device: cuda\n",
      "\n",
      "=== Preparing Data ===\n",
      "\n",
      "=== Dataset Statistics ===\n",
      "Training samples: 50000\n",
      "Validation samples: 10000\n",
      "Test samples: 10000\n",
      "\n",
      "=== Training Configuration ===\n",
      "Initial Learning Rate: 0.001\n",
      "Optimizer: Adam (betas=(0.9, 0.999), eps=1e-08)\n",
      "Learning Rate Scheduler: ReduceLROnPlateau\n",
      " - mode: max (tracking validation accuracy)\n",
      " - factor: 0.1\n",
      " - patience: 3 epochs\n",
      " - min_lr: 1e-6\n",
      "\n",
      "=== Data Augmentation Settings ===\n",
      "Data Augmentation: Disabled\n",
      "Total parameters: 10,624\n",
      "\n",
      "=== Starting Training ===\n",
      "\n",
      "Epoch Summary:\n",
      "Epoch 1/20 (LR=0.001000):\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.9077, Training Accuracy: 90.88% | Validation Loss: 0.7657, Validation Accuracy: 96.46%\n",
      "✓ New best model saved as mnist_model_96.46_20241129_092814.pth\n",
      "\n",
      "Epoch Summary:\n",
      "Epoch 2/20 (LR=0.001000):\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.6478, Training Accuracy: 97.41% | Validation Loss: 0.6656, Validation Accuracy: 97.60%\n",
      "✓ New best model saved as mnist_model_97.60_20241129_092832.pth\n",
      "\n",
      "Epoch Summary:\n",
      "Epoch 3/20 (LR=0.001000):\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.6029, Training Accuracy: 98.04% | Validation Loss: 0.6158, Validation Accuracy: 97.98%\n",
      "✓ New best model saved as mnist_model_97.98_20241129_092850.pth\n",
      "\n",
      "Epoch Summary:\n",
      "Epoch 4/20 (LR=0.001000):\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.5856, Training Accuracy: 98.36% | Validation Loss: 0.5849, Validation Accuracy: 98.71%\n",
      "✓ New best model saved as mnist_model_98.71_20241129_092908.pth\n",
      "\n",
      "Epoch Summary:\n",
      "Epoch 5/20 (LR=0.001000):\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.5769, Training Accuracy: 98.64% | Validation Loss: 0.5634, Validation Accuracy: 98.87%\n",
      "✓ New best model saved as mnist_model_98.87_20241129_092925.pth\n",
      "\n",
      "Epoch Summary:\n",
      "Epoch 6/20 (LR=0.001000):\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.5712, Training Accuracy: 98.75% | Validation Loss: 0.5612, Validation Accuracy: 99.00%\n",
      "✓ New best model saved as mnist_model_99.00_20241129_092943.pth\n",
      "\n",
      "Epoch Summary:\n",
      "Epoch 7/20 (LR=0.001000):\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.5682, Training Accuracy: 98.88% | Validation Loss: 0.5635, Validation Accuracy: 98.97%\n",
      "\n",
      "Epoch Summary:\n",
      "Epoch 8/20 (LR=0.001000):\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.5659, Training Accuracy: 98.88% | Validation Loss: 0.5573, Validation Accuracy: 99.00%\n",
      "\n",
      "Epoch Summary:\n",
      "Epoch 9/20 (LR=0.001000):\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.5628, Training Accuracy: 99.00% | Validation Loss: 0.5537, Validation Accuracy: 99.03%\n",
      "✓ New best model saved as mnist_model_99.03_20241129_093036.pth\n",
      "\n",
      "Epoch Summary:\n",
      "Epoch 10/20 (LR=0.001000):\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.5610, Training Accuracy: 99.00% | Validation Loss: 0.5492, Validation Accuracy: 99.23%\n",
      "✓ New best model saved as mnist_model_99.23_20241129_093054.pth\n",
      "\n",
      "Epoch Summary:\n",
      "Epoch 11/20 (LR=0.001000):\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.5588, Training Accuracy: 99.05% | Validation Loss: 0.5518, Validation Accuracy: 99.16%\n",
      "\n",
      "Epoch Summary:\n",
      "Epoch 12/20 (LR=0.001000):\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.5577, Training Accuracy: 99.07% | Validation Loss: 0.5464, Validation Accuracy: 99.20%\n",
      "\n",
      "Epoch Summary:\n",
      "Epoch 13/20 (LR=0.001000):\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.5566, Training Accuracy: 99.12% | Validation Loss: 0.5506, Validation Accuracy: 99.14%\n",
      "\n",
      "Epoch Summary:\n",
      "Epoch 14/20 (LR=0.001000):\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.5547, Training Accuracy: 99.12% | Validation Loss: 0.5456, Validation Accuracy: 99.22%\n",
      "\n",
      "Epoch Summary:\n",
      "Epoch 15/20 (LR=0.000100):\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.5507, Training Accuracy: 99.27% | Validation Loss: 0.5453, Validation Accuracy: 99.25%\n",
      "✓ New best model saved as mnist_model_99.25_20241129_093223.pth\n",
      "\n",
      "Epoch Summary:\n",
      "Epoch 16/20 (LR=0.000100):\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.5495, Training Accuracy: 99.35% | Validation Loss: 0.5420, Validation Accuracy: 99.30%\n",
      "✓ New best model saved as mnist_model_99.30_20241129_093241.pth\n",
      "\n",
      "Epoch Summary:\n",
      "Epoch 17/20 (LR=0.000100):\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.5499, Training Accuracy: 99.32% | Validation Loss: 0.5434, Validation Accuracy: 99.28%\n",
      "\n",
      "Epoch Summary:\n",
      "Epoch 18/20 (LR=0.000100):\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.5492, Training Accuracy: 99.28% | Validation Loss: 0.5440, Validation Accuracy: 99.29%\n",
      "\n",
      "Epoch Summary:\n",
      "Epoch 19/20 (LR=0.000010):\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.5476, Training Accuracy: 99.38% | Validation Loss: 0.5429, Validation Accuracy: 99.29%\n",
      "\n",
      "Epoch Summary:\n",
      "Epoch 20/20 (LR=0.000010):\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.5487, Training Accuracy: 99.36% | Validation Loss: 0.5416, Validation Accuracy: 99.29%\n",
      "\n",
      "=== Final Evaluation ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|████████████████████████████████████████████████| 79/79 [00:02<00:00, 30.62it/s, loss=0.5415, acc=99.41%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final Test Results:\n",
      "Test Loss: 0.5415\n",
      "Test Accuracy: 99.41%\n",
      "\n",
      "=== Training Complete ===\n"
     ]
    }
   ],
   "source": [
    "train(use_augmentation=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dcd00d1-a8d7-450c-b4bd-d998a8066a85",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49dae42e-7c2c-49c0-80bc-444711b8e416",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pytorch 2.5 (pytorch)",
   "language": "python",
   "name": "pytorch25"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
